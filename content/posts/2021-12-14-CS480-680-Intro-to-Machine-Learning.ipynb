{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f398217",
   "metadata": {},
   "source": [
    "# CS480/680\n",
    "> CS480/680 Intro to Machine Learning - Spring 2019 Course \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,Deep Learning]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8141b624",
   "metadata": {},
   "source": [
    "## Lecture 12 \n",
    " \n",
    "### Gausain process \n",
    " - infinite dimentional gaussian distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da062852",
   "metadata": {},
   "source": [
    "## Lecture 16\n",
    "\n",
    "### Convolution NN\n",
    "\n",
    "- a rule of thumb: to have many layers with smaller filters is better than having one big filter, as going deep captures better features and also uses fewer parameters \n",
    "\n",
    "### Residual Networks\n",
    "\n",
    "- even after using Relu, NN can still suffer from gradient vanishing \n",
    "- the idea in to add skip connections so that we can create shorter paths \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3225c91d",
   "metadata": {},
   "source": [
    "## Lecture 18\n",
    "\n",
    "### LSTM vs GRU vs Attention \n",
    "- LSTM: 3 gates, one for the cell state, one for the input, one for the output\n",
    "- GRU: only two states, one for output, and one for taking weighted probablitiy for the contribution of the input and the hidden state\n",
    "    - takes less parameters\n",
    "        \n",
    "- Attention: at every step of producing the output, create a new context vector that gives more attention to the importat input tokens for this output token\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3019b00",
   "metadata": {},
   "source": [
    "## Lecture 20 \n",
    "\n",
    "### Autoencoder \n",
    "\n",
    "takes different input and generates the same output \n",
    "\n",
    "used in:\n",
    " - compression \n",
    " - denoising \n",
    " - sparse representation\n",
    " - data generation \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35af15",
   "metadata": {},
   "source": [
    "## Lecture 21\n",
    "\n",
    "\n",
    "### Generative models \n",
    " \n",
    "#### Variational autoencoders \n",
    "\n",
    "- idea: train the encoder to sample a fixed distribution , \n",
    "- we want the network to sample a fixed distribution that is close to the distribution of the encoder, so that it generate similar outputs to the input, but not the same \n",
    "\n",
    "#### GANS:\n",
    "\n",
    "- \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fa17ca",
   "metadata": {},
   "source": [
    "## Lecture 22 \n",
    "\n",
    "### Ensemble Learning:\n",
    "\n",
    "\n",
    "   - the idea is to combine the hypothesis of several models to produce a better one \n",
    "#### Bagging \n",
    "       - choose the class the majority votes \n",
    "#### Weighted majority\n",
    "    - decrease the weight of corrlelated hypothesis \n",
    "    - increase the weight of good hypothesis \n",
    "#### Boosting\n",
    "\n",
    "\n",
    "    - idea: when an instance is missclassified  by hypothesis, increase its weight so that the next hypothesis is more likely to classify  it correctly \n",
    "    \n",
    "    - can boost a weak learner \n",
    "    \n",
    "    - makes weighted training set, so that it can focus on missclassified examples \n",
    "    \n",
    "\n",
    "    - at the end generate weighted hypotheses based on the acc of each hypothsis \n",
    "    \n",
    "    - Advantages:\n",
    "        - no need to learn perfect hypothesis \n",
    "        - can boost any weak learning algo\n",
    "        - boosting is very simple \n",
    "        - has good generalization \n",
    "        \n",
    " **Netflex challenge 2006**\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4482584d",
   "metadata": {},
   "source": [
    "### Lecture 23\n",
    "\n",
    "\n",
    "#### Normalizing flows\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e80494",
   "metadata": {},
   "source": [
    "### Lecture 24\n",
    "\n",
    "#### Gradient boosting \n",
    "- boosting for regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48759589",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
