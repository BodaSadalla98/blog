<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>python - Category - Boda Blog</title><link>https://bodasadalla98.github.io/blog/categories/python/</link><description>python - Category - Boda Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bodasadallah@gmail.com (Boda Sadallah)</managingEditor><webMaster>bodasadallah@gmail.com (Boda Sadallah)</webMaster><lastBuildDate>Mon, 23 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://bodasadalla98.github.io/blog/categories/python/" rel="self" type="application/rss+xml"/><item><title>Programming Facts</title><link>https://bodasadalla98.github.io/blog/programming_facts/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/programming_facts/</guid><description><![CDATA[Intro this is intended to be some kind or reference to go through whenever I face some kind of bug, or error, that I don&rsquo;t know how to solve.usually these kinds of error that doesn&rsquo;t make sense, or we don&rsquo;t know the cause of them, and the worst it, that we don&rsquo;t find many ppl facing the same issue, so internet can&rsquo;t be so useful then. I learned theses facts the hard way, spending so much time trying to figure out the root of the issue.]]></description></item><item><title>Deep Learning Papers Summarization</title><link>https://bodasadalla98.github.io/blog/papers/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/papers/</guid><description>Decoupled Neural Interfaces using Synthetic Gradients In NN, the training process, has 3 bottle-necks forward lock: you need to calculate teh output of the previous layer before you can can go into next layer in forward pass backward pass: the same, but for backward propagation weights lock: you can&amp;rsquo;t update weights unless you do for weights in next layer the paper trying to unlock these bootle-necks by decoupling each layer, to be sufficient alone it does that by introducing, a Synthetic Gradient Model, that can predict the gradient for the current layer, without waiting for the gradient of the next layer this was we can calculate gradient and update weights as soon as we calculate the activation of the current layer Synthetic Gradient Model can be just a simple NN that is trained to output the gradient of the layer</description></item><item><title>Stanford CS224N NLP with Deep Learning</title><link>https://bodasadalla98.github.io/blog/stanford_nlp_cs224n/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/stanford_nlp_cs224n/</guid><description>Post for Stanford NLP Course</description></item><item><title>Applied Deep Learning</title><link>https://bodasadalla98.github.io/blog/applied_deep_learning/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/applied_deep_learning/</guid><description>Reference Course Repo with the slides, and course info Deep Learning overview we can look at deep learning as an algorithm that writes algorithms, like a compiler in this case the source code would be the data: (examples/experiences) excutable code would be the deployable model Deep: Functions compositions $ fl f{l-1} &amp;hellip;. f_1$
Learning: Loss, Back-propagation, and Gradient Descent
$ L(\theta) \approx J(\theta)$ &amp;ndash;&amp;gt; noisy estimate of the objective function due to mini-batching.</description></item><item><title>NLP Specialization</title><link>https://bodasadalla98.github.io/blog/coursera_nlp_specialization/</link><pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/coursera_nlp_specialization/</guid><description>Course 1: Classification and vector Spaces Weak 4 Hashing We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space Locality senstive hashing the idea is to put items that are close in the vector space, in the same hashing buckets
we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly</description></item><item><title>Python</title><link>https://bodasadalla98.github.io/blog/python/</link><pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/python/</guid><description>Inheritance If you have inherited from parent class then you should call the parent class constructor if you overload it, or simply doesn&amp;rsquo;t overload it Ex:
1 2 3 4 5 6 7 8 9 10 class parent: def __init__: class child: def __init__: super().__init__() class child2: ## or simply don&amp;#39;t override the constructor and use the parent one Multiple Inheritance when we inherit from two or more classes, whatever class we inherited first(typed first in the list), would be the one to have pariority</description></item><item><title>Scribbles</title><link>https://bodasadalla98.github.io/blog/scribbles/</link><pubDate>Wed, 27 Oct 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/scribbles/</guid><description><![CDATA[Transformers To deal with sequential data we have to options:
1-D convolution NN processing can be parallel not practical for long sequences Recurrent NN can&rsquo;t happen in prallel have gradient vanshing problem of the squence becomes so long we have bottleneck at the end of the encoder RNN with attention mechanism to solve the bottleneck problem, we make Encoder-Decoder attention Decoder utilzes: context vector weighted sum of hidden states (h1,h2, &hellip; ) from the encoder Transformers Encoder first we do input embedding, and positional embedding in self attention: we multiply q,w,v by a matrix to do lenear transformation self attentoion: k * q &ndash;&gt; scaling down &ndash;&gt; softmax &ndash;&gt; * v multi-head attention works as we use many filters in CNN in wide attention: it takes every word and spread it multi-head attention in narrow attention: it take every word and split it up across the multi-head but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN?]]></description></item><item><title>Distributed Training in PyTorch</title><link>https://bodasadalla98.github.io/blog/distributed_training/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Baraa, Boda</author><guid>https://bodasadalla98.github.io/blog/distributed_training/</guid><description>Distributed Training Why? Need more compute power to process large batches in parallel (DDP)
Uses collective communication Large model that couldnâ€™t be fit in memory of one GPU (RPC)
Uses P2P communication All of the above XD
DDP in Pytorch Every GPU has a model replica, controlled by a process. Every process fetches different batch of data. Forward. Overlapping between computation of and communication(broadcast - allreduced) of gradient. Validation 4 steps-recipe to Distributed Training Initialize Distributed Group 1 init_process_group(backend=&amp;#39;nccl&amp;#39;) Data Local Training 1 2 3 4 # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size) Distributed Training 1 2 3 4 5 6 7 8 9 10 # Create distributed sampler pinned to rank sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) # May be True # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, # Must be False!</description></item></channel></rss>