<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>autograd - Category - Boda Blog</title><link>https://bodasadalla98.github.io/blog/categories/autograd/</link><description>autograd - Category - Boda Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bodasadallah@gmail.com (Boda Sadallah)</managingEditor><webMaster>bodasadallah@gmail.com (Boda Sadallah)</webMaster><lastBuildDate>Tue, 10 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://bodasadalla98.github.io/blog/categories/autograd/" rel="self" type="application/rss+xml"/><item><title>A glimpse into PyTorch Autograd internals</title><link>https://bodasadalla98.github.io/blog/pytorch_internals/</link><pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/pytorch_internals/</guid><description><![CDATA[Intro Here, we are going to discuss the internals of PyTorch Autograd module. The most of us don&rsquo;t have to know about this. I was the same till I came across this error:
1 unsupported operand type(s) for *: &#39;float&#39; and &#39;NoneType&#39; This came from executing the following code:
1 2 3 4 5 6 import torch a = torch.tensor(5.0, requires_grad=True) * 0.1 b = torch.tensor(2.0, requires_grad=True) c = a + b c.]]></description></item></channel></rss>