<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>pytorch - Tag - Boda Blog</title><link>https://bodasadalla98.github.io/blog/tags/pytorch/</link><description>pytorch - Tag - Boda Blog</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bodasadallah@gmail.com (Boda Sadallah)</managingEditor><webMaster>bodasadallah@gmail.com (Boda Sadallah)</webMaster><lastBuildDate>Tue, 10 Jan 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://bodasadalla98.github.io/blog/tags/pytorch/" rel="self" type="application/rss+xml"/><item><title>A glimpse into PyTorch Autograd internals</title><link>https://bodasadalla98.github.io/blog/pytorch_internals/</link><pubDate>Tue, 10 Jan 2023 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/pytorch_internals/</guid><description><![CDATA[Intro Here, we are going to discuss the internals of PyTorch Autograd module. The most of us don&rsquo;t have to know about this. I was the same till I came across this error:
1 unsupported operand type(s) for *: &#39;float&#39; and &#39;NoneType&#39; This came from executing the following code:
1 2 3 4 5 6 import torch a = torch.tensor(5.0, requires_grad=True) * 0.1 b = torch.tensor(2.0, requires_grad=True) c = a + b c.]]></description></item><item><title>Distributed Training in PyTorch</title><link>https://bodasadalla98.github.io/blog/distributed_training/</link><pubDate>Thu, 11 Aug 2022 00:00:00 +0000</pubDate><author>Baraa, Boda</author><guid>https://bodasadalla98.github.io/blog/distributed_training/</guid><description>Distributed Training Why? Need more compute power to process large batches in parallel (DDP)
Uses collective communication Large model that couldnâ€™t be fit in memory of one GPU (RPC)
Uses P2P communication All of the above XD
DDP in Pytorch Every GPU has a model replica, controlled by a process. Every process fetches different batch of data. Forward. Overlapping between computation of and communication(broadcast - allreduced) of gradient. Validation 4 steps-recipe to Distributed Training Initialize Distributed Group 1 init_process_group(backend=&amp;#39;nccl&amp;#39;) Data Local Training 1 2 3 4 # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size) Distributed Training 1 2 3 4 5 6 7 8 9 10 # Create distributed sampler pinned to rank sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank, shuffle=True) # May be True # Wrap train dataset into DataLoader train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, # Must be False!</description></item></channel></rss>