[{"categories":null,"content":"sd fsdfsdf ","date":"2022-07-02","objectID":"/blog/first_post/:0:0","tags":null,"title":"First_post","uri":"/blog/first_post/"},{"categories":null,"content":"This is just a test ","date":"2022-07-02","objectID":"/blog/first-post/:0:0","tags":null,"title":"First Post","uri":"/blog/first-post/"},{"categories":null,"content":"MIT 18.06 Linear Algebra course MIT 18.06 Linear Algebra course toc: true badges: true comments: true categories: [jupyter,MIT,Linear Algebra, Math] ","date":"0001-01-01","objectID":"/blog/1806-mit/:0:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 1 We learn about the big picture behind mutplication of matrix and vector we learn about the row picture and column picture ","date":"0001-01-01","objectID":"/blog/1806-mit/:1:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 2 we learned about elimination method to solve a system of equations ","date":"0001-01-01","objectID":"/blog/1806-mit/:2:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 3 in this lecture we learned about matrics multiplication: we can do that in five ways: row * col ==\u003e gives an entry (1 cell) col * row ==\u003e sum ( r1 * c1 , r2 * c2, etc) by columns ==\u003e A * c1 = combination of A columns by columns ==\u003e r1 * B = combination of A B rows by blocks ==\u003e A (A1,A2,A3,A4) * B (B1,B2,B3,B4) = C1 = (A1* B1 + A2 * B3) and so on then we learned about gausian-Jordan elimination to find the matrix inverse [A | I] ==\u003e we apply elimination till we get [ I | A-1 ] ","date":"0001-01-01","objectID":"/blog/1806-mit/:3:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 4 in this lecture we learn about A= L U, where L is E^ -1, and whats special about this is that it has all multipliers in the lower triangular with ones on the diagonal ","date":"0001-01-01","objectID":"/blog/1806-mit/:4:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 5 we continued a little with permutations and moved to vector spaces we learned about sub spaces and columns spaces ==\u003e which is u take the columns of the matrix and all its combinations and that creates a plane through origin making a columns space ","date":"0001-01-01","objectID":"/blog/1806-mit/:5:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 6 In this lecture we continued about columns spaces and that we build those up by taking the combinations of all columns. Then we learned about null spaces while are sub spaces of X that satisfies A X = 0 ","date":"0001-01-01","objectID":"/blog/1806-mit/:6:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 7 in this lecture we continued about null space then we learned about the special solution, where we assume the variables of the free vector then get the special solution finally we learned about the reduced form where R = [ I F 0 0 ] and the null matrix is [ -F I ] then R N = 0 ","date":"0001-01-01","objectID":"/blog/1806-mit/:7:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 8 in this lecture we expanded to talk about A x = b and we find the whether there’s a solution to the equation or not depends on the rank of the matrix also we get the Xcomplete = Xparticular + Xnull space and we get particular soln by putting all free variables = zero ","date":"0001-01-01","objectID":"/blog/1806-mit/:8:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 9 in this lecture we learned about independent columns and how they make a space, we also learned about Basis and what are two conditions for it rank(A) = number of pivot columns of A = dimension of C(A). dimension of N(A) = number of free variables = n − r, ","date":"0001-01-01","objectID":"/blog/1806-mit/:9:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 10 In this lecture we learned about the four subspaces we also started in matrix space M ","date":"0001-01-01","objectID":"/blog/1806-mit/:10:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 11 We learned about matrix space we take introduction about graph ","date":"0001-01-01","objectID":"/blog/1806-mit/:11:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 12 in this lecture we learned about graphs and how to represent them with matrices, then we applied that to electrical system and applied kerchofs law ","date":"0001-01-01","objectID":"/blog/1806-mit/:12:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 13 Quiz 1 review ","date":"0001-01-01","objectID":"/blog/1806-mit/:13:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 14 in this lecture learned about othrignilaity of the four vector spaces and what does it means ","date":"0001-01-01","objectID":"/blog/1806-mit/:14:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 15 in this lecture we learned about projection of matrices into subspaces ","date":"0001-01-01","objectID":"/blog/1806-mit/:15:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 16 we got example explaning the projection into subspaces and how to get the best fit using the least square error ","date":"0001-01-01","objectID":"/blog/1806-mit/:16:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 17 in this lecture le learned about orthonormal vectors and their special features and we learn how to produce them from any independent vectors using gram-schmeit ","date":"0001-01-01","objectID":"/blog/1806-mit/:17:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 18 Propertise of determants ","date":"0001-01-01","objectID":"/blog/1806-mit/:18:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 19 det I =1 sign reverses with each row or colums exchange det is linear in each row seperately ","date":"0001-01-01","objectID":"/blog/1806-mit/:19:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Big Det Formula for a N * N matrix, we calc the sum of N! terms $$detA=\\sum_{i=1}^ N a1\\alphaa2\\betaa3\\gamma*an\\omega$$ (where $\\alpha, \\beta, … \\omega $) = perm of (1,2,3, …, N ) ","date":"0001-01-01","objectID":"/blog/1806-mit/:19:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Cofactors cofactor of aij = Cij = +/- det of ( n-1 matrix with column j, and row i erased ) it is plus if i+j is even, minus if i+j is odd cofactor formula (along row 1) det A = a11 C11 + a12 C12 + .... + a1n C1n ","date":"0001-01-01","objectID":"/blog/1806-mit/:19:2","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 20 $ A^{-1} = 1/detA * C^T$ where C is the cofactors matrix ","date":"0001-01-01","objectID":"/blog/1806-mit/:20:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Cramers rule A x = b x= A^ (-1) b = 1/detA C^T b X_j = detB_j / detA where B_j is A matrix with column j replaced by b ","date":"0001-01-01","objectID":"/blog/1806-mit/:20:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Det A = Volume detA = volume of the shape created by making an edge from each of the rows ","date":"0001-01-01","objectID":"/blog/1806-mit/:20:2","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 21 Eigenvalues and Eigenvectors Eigenvectors: Ax is prallel to x ==\u003e Ax = $\\lambda$x lambda ia the eigen values if we have a plane: any x in the plane: Px= x ==\u003e x is eigenvector and lambda = 1 any x perpendicular to plane Px = 0 ==\u003e x is eigen vector and lambda = 0 Fact: the sum of the eigenvalues = the sum of the diagonal of A ","date":"0001-01-01","objectID":"/blog/1806-mit/:21:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 22: Diagnolization to get power of matrix $A^k$ first get the eigenvalues and vectors for A then compute $A = S \\lambda S^{-1}$ where S is the eigenvector matrix, and Lambda is diagonal matrix of the eigenvalues then $A^k = S * \\lambda^k*S^{-1}$ ","date":"0001-01-01","objectID":"/blog/1806-mit/:22:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 23 for the diffrential equations: 1- Stability if lambda \u003c 0 ==\u003e u(t) –\u003e 0 2- Steady state if lambda1 = 0, lambda2 \u003c 0 3- Blowup if any lambda \u003e 0 ","date":"0001-01-01","objectID":"/blog/1806-mit/:23:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 24 ","date":"0001-01-01","objectID":"/blog/1806-mit/:24:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Markov Matrix 1- All entries \u003e= 0 2- The sum of every column is 1 3- lambda = 1 is eigenvalue 4- all othe lambda \u003c 1 5- eigenvector values \u003e= 0 ","date":"0001-01-01","objectID":"/blog/1806-mit/:24:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Fourier series integration of ( f(x) g(x) dx ) from 0 to 2pi = 0 ","date":"0001-01-01","objectID":"/blog/1806-mit/:24:2","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 25 ","date":"0001-01-01","objectID":"/blog/1806-mit/:25:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Symmetric matrices $ A = A^T$ the eigenvalues are real the eigenvectors are perpendicular usual case: $A = S \\lambda S^{-1}$ symmetric case: we have orthonormal eigenvectors $A = Q \\lambda Q^{-1} = Q \\lambda Q^{-T} $ ","date":"0001-01-01","objectID":"/blog/1806-mit/:25:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Every symmetric matrix is a combination of perp. projection matrices ","date":"0001-01-01","objectID":"/blog/1806-mit/:25:2","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Signs of pivots are the same as the sign of the eigenvalues product of pivots = product of eigenvalues = det of matrix ","date":"0001-01-01","objectID":"/blog/1806-mit/:25:3","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Positive definite symmetric matrix all eigenvalues are positives all pivots are positive det is positive as it’s the product of the eigenvalues also all sub detemants are positive ( determants of lower matrices ) if S is pos. definite ==\u003e $ X^TSX$ must be positive ","date":"0001-01-01","objectID":"/blog/1806-mit/:26:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 26 ","date":"0001-01-01","objectID":"/blog/1806-mit/:27:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Complex Matrices we wanna utilize tha fact that $\\bar{Z^T}*Z = \\left|{Z}\\right|^2$ Hermitian is biscally the conj. and transpose ==\u003e $Z^H = \\bar{Z^T}$ Hermitian Matricies : $A^H = A$ perpendicular: q1, q2, …, qn $\\bar{qi}^T * qj = 0 if i!=j, 1 if i=j $ $Q^H*Q = I$ ","date":"0001-01-01","objectID":"/blog/1806-mit/:27:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Fourier Matrix a matrix with entries are powers of some number W. where $W^n = 1$ $ F^H*F = I$ ","date":"0001-01-01","objectID":"/blog/1806-mit/:27:2","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Fast fourier transform reduces complexity from $N^2 to N log(N)$ $W_{2n}^2 = W_n ==\u003e W_4^2 = W_2$ ","date":"0001-01-01","objectID":"/blog/1806-mit/:27:3","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 27 when det= 0 ==\u003e then the matrix is positive semi-definite f(x1,x2,x3…,xn) ==\u003e min when the matrix of second derivatives is positive definite the eigenvalues tells us the length of the axis of the shape crated by cutting through the shape of the $X^T A X $ the direction of the eigenvectors is the direction of the axis of that shape ","date":"0001-01-01","objectID":"/blog/1806-mit/:28:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 28 A is a m by n matrix ==\u003e $ A^T*A$ is a positive definite symmetric matrix ","date":"0001-01-01","objectID":"/blog/1806-mit/:29:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"similar matrices A and B are similar means: for some M ==\u003e $B = M^{-1}AM$ Similar matrices has the same eigenvalues if the eigenvalues are unique the eigenvector of B is $M^{-1} * (eigenvector ofA)$ ","date":"0001-01-01","objectID":"/blog/1806-mit/:29:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Jordan form every square A is similar to Jordan matrix J every jordan block has one eigenvaector the number of jordan blocks = number of eigenvectors ","date":"0001-01-01","objectID":"/blog/1806-mit/:29:2","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 29 eigenvalues of (AB) = eigenvalues of (BA) ","date":"0001-01-01","objectID":"/blog/1806-mit/:30:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Singular value composition (SVD) $Av = \\sigma u $ $ A = u \\sigma v^T = u \\sigma v^{-1}$ $A^T A = v \\sigma^T u^Tu /sigma v^T = v \\sigma^2 v^T$ $A A^T= u \\sigma^T v^Tv /sigma u^T = u \\sigma^2 u^T$ ","date":"0001-01-01","objectID":"/blog/1806-mit/:30:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 30 ","date":"0001-01-01","objectID":"/blog/1806-mit/:31:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Linear transformation examples: projection, rotation if u know what transfotmation does to the basis of a plane, then u know what it does to every vector in the plane every $ v = c_1 v_1 + c_2 v_2 + … + c_n v_n$ then $T(v) = c_1 T(v_1) + …. + c_n T(v_n)$ coordinates come from a basis (think of basis like the X-Y-Z axis so we have a basis for the input and a basis for the output Rule to find matrix A, given the input and output basis : input basis: v1 ===\u003e vn output basis: w1 ===\u003e wm 1st column of A : write T(v1) = a11 w1 + a21 w2 + … + am1 wm 2nd column of A : write T(v2) = a12 w1 + a22 w2 + … + am2 wm repeat that for the n columns A * (input coordinates) = (output coordinates) ","date":"0001-01-01","objectID":"/blog/1806-mit/:31:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 31 ","date":"0001-01-01","objectID":"/blog/1806-mit/:32:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"change of basis we have a new basis vectors and we wanna change to the new basis W A = c * W ==\u003e c = W^-1 * A when we change the basis, every vector would have new coordinates ==\u003e old coordinates = new basis * new coordinates ==\u003e x = W c ","date":"0001-01-01","objectID":"/blog/1806-mit/:32:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecture 32 ","date":"0001-01-01","objectID":"/blog/1806-mit/:33:0","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"2-sided inverse $A A^{-1} = I = A^{-1} A$ r = m = n ==\u003e full rank ","date":"0001-01-01","objectID":"/blog/1806-mit/:33:1","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"left inverse full column rank r = n nullspace = 0 then $A^T A $ is invertable $A^{-1}_{left} = (A^T A)^{-1} A^T $ $A^{-1}_{left} A = I$ ","date":"0001-01-01","objectID":"/blog/1806-mit/:33:2","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"right inverse full row rank r = m n-m free variables left nullspace = 0 ","date":"0001-01-01","objectID":"/blog/1806-mit/:33:3","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"pseudo inverse $A^+$ $A^+ = v*\\sigma^{-1}*u^T$ ","date":"0001-01-01","objectID":"/blog/1806-mit/:33:4","tags":null,"title":"","uri":"/blog/1806-mit/"},{"categories":null,"content":"Lecute 34 no solution ==\u003e rank \u003c m has one solution ==\u003e there’e no null space ==\u003e rank = n a matrix is invertable when there’s no null space ==\u003e r = n ==\u003e indep. columns positive definite matrix must have full rank ==\u003e has no null space positive def is invertable the matrix has soln of any c when the matrix has full row rank matix with orthogonal eigen vectors : symmetric matrices skew-symmetric orthogonal matrices in markov matrix the eigen values are one, and some sother values less than one $k_m$ and m goes to infinity is the steady state we ge the eigenvector that corresponts to eigenvalue one asn multiply it with c, and notes that the sum of u is alwayes the same, so the sum of u0 is the sum of uk, so look what c achieves that ","date":"0001-01-01","objectID":"/blog/1806-mit/:33:5","tags":null,"title":"","uri":"/blog/1806-mit/"}]