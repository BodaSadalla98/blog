{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman.atef/anaconda3/envs/dl/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tensor(name,t):\n",
    "    print(f'--------------------------- {name} --------------------')\n",
    "    print(f'{t.data=}')\n",
    "    print(f'{t.grad=}')\n",
    "    print(f'{t.grad_fn=}')\n",
    "    print(f'{t.is_leaf=}')\n",
    "    print(f'{t.requires_grad=}')\n",
    "    print(f'------------------------------------------------------')\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- a --------------------\n",
      "t.data=tensor(0.5000)\n",
      "t.grad=None\n",
      "t.grad_fn=None\n",
      "t.is_leaf=True\n",
      "t.requires_grad=True\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor(5.0) * 0.1\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "a.requires_grad = True\n",
    "c = a + b\n",
    "c.backward()\n",
    "with torch.no_grad():\n",
    "    a += + 0.1 * a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------- a --------------------\n",
      "t.data=tensor(2.)\n",
      "t.grad=None\n",
      "t.grad_fn=None\n",
      "t.is_leaf=True\n",
      "t.requires_grad=False\n",
      "------------------------------------------------------\n",
      "--------------------------- w --------------------\n",
      "t.data=tensor(3.)\n",
      "t.grad=tensor(2.)\n",
      "t.grad_fn=None\n",
      "t.is_leaf=True\n",
      "t.requires_grad=True\n",
      "------------------------------------------------------\n",
      "--------------------------- b --------------------\n",
      "t.data=tensor(2.)\n",
      "t.grad=tensor(1.)\n",
      "t.grad_fn=None\n",
      "t.is_leaf=True\n",
      "t.requires_grad=True\n",
      "------------------------------------------------------\n",
      "--------------------------- c --------------------\n",
      "t.data=tensor(8.)\n",
      "t.grad=None\n",
      "t.grad_fn=<AddBackward0 object at 0x7f25fadfb940>\n",
      "t.is_leaf=False\n",
      "t.requires_grad=True\n",
      "------------------------------------------------------\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "a leaf Variable that requires grad is being used in an in-place operation.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/data/boda/blog/content/posts/pytorch_internals/play.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/data/boda/blog/content/posts/pytorch_internals/play.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m print_tensor(\u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m,b)\n\u001b[1;32m     <a href='vscode-notebook-cell:/data/boda/blog/content/posts/pytorch_internals/play.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m print_tensor(\u001b[39m'\u001b[39m\u001b[39mc\u001b[39m\u001b[39m'\u001b[39m, c)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/data/boda/blog/content/posts/pytorch_internals/play.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m w \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m   \u001b[39m0.1\u001b[39m\u001b[39m*\u001b[39mw\u001b[39m.\u001b[39mgrad \n\u001b[1;32m     <a href='vscode-notebook-cell:/data/boda/blog/content/posts/pytorch_internals/play.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m print_tensor(\u001b[39m'\u001b[39m\u001b[39ma\u001b[39m\u001b[39m'\u001b[39m,a)\n\u001b[1;32m     <a href='vscode-notebook-cell:/data/boda/blog/content/posts/pytorch_internals/play.ipynb#W2sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m print_tensor(\u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m,w)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: a leaf Variable that requires grad is being used in an in-place operation."
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0)\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "for i in range(2):\n",
    "    c = a * w +  b\n",
    "\n",
    "    c.backward()\n",
    "\n",
    "    # d = torch.tensor(3.0,requires_grad=True)\n",
    "    # e = c * d \n",
    "    print_tensor('a',a)\n",
    "    print_tensor('w',w)\n",
    "    print_tensor('b',b)\n",
    "    print_tensor('c', c)\n",
    "\n",
    "    w +=   0.1*w.grad \n",
    "\n",
    "    print_tensor('a',a)\n",
    "    print_tensor('w',w)\n",
    "    print_tensor('b',b)\n",
    "    print_tensor('c', c)\n",
    "\n",
    "# print_tensor('d',d)\n",
    "# print_tensor('e',e)\n",
    "# e.backward()\n",
    "# print_tensor('a',a)\n",
    "# print_tensor('b',b)\n",
    "# print_tensor('c', c)\n",
    "# print_tensor('d',d)\n",
    "# print_tensor('e',e)\n",
    "\n",
    "\n",
    "# a = a * 0.1\n",
    "# print_tensor('a',a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "tensor(1.)\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    a = torch.tensor(5.0, requires_grad=True) * .01\n",
    "print(a.is_leaf)\n",
    "\n",
    "a.requires_grad = True\n",
    "b = torch.tensor(2.0,  requires_grad=True)\n",
    "c = a + b\n",
    "d = c * 5\n",
    "\n",
    "print(a.is_leaf)\n",
    "c.backward()\n",
    "print(a.is_leaf)\n",
    "\n",
    "\n",
    "print(a.grad)\n",
    "print(b.grad)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    a += + 0.1 * a.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor(2.0)\n",
    "b = torch.tensor(2.0)\n",
    "c = a * b\n",
    "print(c.is_leaf)\n",
    "\n",
    "c.requires_grad = True\n",
    "print(c.is_leaf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: y = 1.5635324716567993 + -0.32995063066482544 x + -2.028961420059204 x^2 + 0.2935994267463684 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(id(a))\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "\n",
    "    a = a - learning_rate * a.grad\n",
    "    b = b - learning_rate * b.grad\n",
    "    c = c - learning_rate * c.grad\n",
    "    d = d - learning_rate * d.grad\n",
    "    # Manually zero the gradients after updating weights\n",
    "    a.grad = None\n",
    "    b.grad = None\n",
    "    c.grad = None\n",
    "    d.grad = None\n",
    "\n",
    "    break\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Input\n",
    "a = torch.tensor(2.0)\n",
    "# weight\n",
    "w = torch.tensor(3.0, requires_grad=True)\n",
    "# bias\n",
    "b = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "for _ in range(10):\n",
    "    # forward pass\n",
    "    c = a * w +  b\n",
    "    c += 1\n",
    "    # backward pass\n",
    "    c.backward()\n",
    "    # weight update\n",
    "    with torch.no_grad():\n",
    "        w+=  0.1*w.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "torch.manual_seed(0)\n",
    "\n",
    "a = torch.randn( (), requires_grad=True)\n",
    "initial_address = a.data_ptr()\n",
    "with torch.no_grad():\n",
    "    a += 5  #in-place operation\n",
    "print(initial_address == a.data_ptr())\n",
    "a = a + 5 #out-of-place operation\n",
    "print(initial_address == a.data_ptr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64b178eb49eecde524ce545b08b9f340bede480db49f6e9a9fc4b812ce5e0df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
