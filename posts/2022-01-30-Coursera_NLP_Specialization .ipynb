{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f901bb86",
   "metadata": {},
   "source": [
    "# NLP Specialization \n",
    "> Notebook for coursera's NLP Specialization \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,deeplearning,python,NLP]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5efd2e8",
   "metadata": {},
   "source": [
    "# Course 1: Classification and vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea02882",
   "metadata": {},
   "source": [
    "## Weak 4 \n",
    "\n",
    "### Hashing \n",
    "\n",
    "* We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space\n",
    "\n",
    "#### Locality senstive hashing \n",
    "* the idea is to put items that are close in the vector space, in the same hashing buckets \n",
    "* we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly \n",
    "\n",
    "* but how can we be sure, that the planes that we chose are the perfect set to seperate our space, we can't be sure of that, so we create a multi sets of random planes and every set would get us a different way to seperate our words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086b8594",
   "metadata": {},
   "source": [
    "# Course 2: Probabilstic Models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22215003",
   "metadata": {},
   "source": [
    "## Week 1: Autocorrect\n",
    "\n",
    "* to make a simple auto-correction system, you need to perform 4 simple steps:\n",
    "    - you need to identify the miss spelled words\n",
    "    - get the n edit distance correct words\n",
    "    - filter these candidates for correct words in the dictionary\n",
    "    - change miss spelled word with one that has the highest probability\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265a6b25",
   "metadata": {},
   "source": [
    "## Week 2: POS using Viterbi algorithm\n",
    "\n",
    "* Part of Speech Tagging (POS) is the process of assigning a part of speech to a word\n",
    "* You can use part of speech tagging for: \n",
    "    - Identifying named entities\n",
    "    - Speech recognition\n",
    "    - Co-reference Resolution\n",
    "* You can use the probabilities of POS tags happening near one another to come up with the most reasonable output.\n",
    "\n",
    "### Markov Chains:\n",
    "* You can use Markov chains to identify the probability of the next word.\n",
    "* calculate transmission probability\n",
    "* calculate emission probability\n",
    "\n",
    "### Viterbi algorithm\n",
    "* calculates a probability for each possible path\n",
    "* a probability for a given state, is the (transition probability * emission probability)\n",
    "* total probability of the path, is to product of all states probabilities \n",
    "* we use a top down dynamic programming algorithm to build the paths matrix \n",
    "* we use sum of logs instead of product of probabilities, to avoid converging to zero values  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfb3f9b",
   "metadata": {},
   "source": [
    "## Week3 Autocomplete\n",
    "\n",
    "### N-Gram models:\n",
    "* it's a language model that predicts probabilities of sentences depending on the probabilities of their N-grams \n",
    "* to capture the context of beginning end ending of the sentences, we add start and end tokens to each sentence\n",
    "### Preplexity:\n",
    "* a measure to calculate how complex a sentence is\n",
    "* humans type low preplex sentences\n",
    "\n",
    "### out of vocab words and smoothing\n",
    "* we can add UNK token for unseen words in the vocab\n",
    "* we can apply smoothing of interpolation for unseen Ngrams "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571f9b9d",
   "metadata": {},
   "source": [
    "## Week4: Word vectors using Bag of Words method\n",
    "\n",
    "* we can use self-supervised learning in predicting the next word, to learn the weight matrix\n",
    "\n",
    "* word2vec\n",
    "    - continuous bag-of-words \n",
    "        - predicts a word in context\n",
    "    - continuous skip-gram\n",
    "        - tries to predict the words surrounding input word\n",
    "* Global Vectors (GloVe)\n",
    "    - factorizes the corpus word co-occurrence matrix\n",
    "* fastText\n",
    "    - based on skip-gram model\n",
    "    - support out-of-vocab words \n",
    "* Advanced word embedding methods\n",
    "    - Deep Learning, contextual embedding \n",
    "    - BERT\n",
    "    - ELMO \n",
    "    - GPT-2\n",
    "### Continuous Bag-of-Words Model\n",
    "* you choose a center word, and a context words around it, and try to predict the centered word\n",
    "* we model the words by one-hot encoding \n",
    "* then we prepare the input training example feature to be an average of the one-hot vectors of the context words, and the label would be one-hot vector of the center word\n",
    "* after training the model, the word embedding is one of the weight matrices, or an average of them\n",
    "#### Evaluations\n",
    "-   Intrinsic evaluation\n",
    "    - test the relationships between words\n",
    "    - Analogies\n",
    "    - Clustering\n",
    "    - Visualizing\n",
    "- Extrinsic evaluation \n",
    "    - test the embedding on the end task you want to perform (ex. Sentiment Analysis)\n",
    "    - evaluates actual usefulness of embeddings\n",
    "    - time consuming\n",
    "    - more difficult to troubleshoot\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
