{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "908c9a47",
   "metadata": {},
   "source": [
    "# Applied Deep Learning\n",
    "> Applied Deep Learning [Course](https://github.com/maziarraissi/Applied-Deep-Learning) \n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter,deeplearning,python]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027d97f6",
   "metadata": {},
   "source": [
    "##  Reference \n",
    "[Repo with the slides, and course info](https://github.com/maziarraissi/Applied-Deep-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae636b1",
   "metadata": {},
   "source": [
    "## Deep Learning overview\n",
    "* we can look at deep learning as an algorithm that writes algorithms, like a compiler\n",
    " - in this case the source code would be the data: (examples/experiences)\n",
    " - excutable code would be the deployable model \n",
    "\n",
    " * Deep: Functions compositions  $ f_l f_{l-1} .... f_1$\n",
    " * Learning: Loss, Back-propagation, and Gradient Descent\n",
    "\n",
    " * $ L(\\theta) \\approx J(\\theta)$ --> noisy estimate of the objective function due to mini-batching. That's why we call it stochastic Gradient Descent \n",
    " * why do we use the first order derivative, not the second order one (the hessian), because order of first order derivative is N, but for the hessian it's N*N, so it's computationally expensive and slow \n",
    " ### Optimizers\n",
    " * to make gradient descent faster, we can add momentum to it.\n",
    "  * another way is to use Nesttov Accelerated Gradient: the idea is to look ahead while computing the gradient, so we can add that to the momentum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b3fe29",
   "metadata": {},
   "source": [
    "  * RMSprop: A mini-batch version of rprop method. the original rprop can't work with mini batches, as it doesn't consider the magnitude of the gradient, but only the sign of it, and it would multiply the gradient by a fixed factor every time depending on the sign. \n",
    "\n",
    "  ![](assets/Applied_deep_learning/rprop.png)\n",
    "\n",
    "\n",
    "  * Nestrov adaptive optimizer: the main idea is that we know that we gonna update the weights according to our average velocity so far, and also our gradient, but this can cause us to over shoot as we have a huge velocity moving down the hill, so why not update first the weights according to our velocity and see where that gets us (the look ahead term), and then we update the weights according to the gradient there "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e06d5f",
   "metadata": {},
   "source": [
    "* Adam:\n",
    "    - can take different time steps for each paramater (Adaptive steps) (took concepts from Adadelta)\n",
    "    - can also has momentum for all parameter wich can lead to faster convergence\n",
    "* Nadam: Just like Adam but with added nestrov acceleration look ahead functionality so we can slow down we go near the goal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53571c3",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "* A simple method to prevent the NN from overfitting \n",
    "* CNNS are less prune to overfitting becaue the weight sharing idea, that we have a set of filters fot the entire image \n",
    "* you can look at dropout as a smart way of ensembling, as it combines exponentially many different networks architectures effienctly. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79537fbe",
   "metadata": {},
   "source": [
    "# Computer Vision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e67da4c",
   "metadata": {},
   "source": [
    "## Image Classification \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af18a38c",
   "metadata": {},
   "source": [
    "### Large Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82030f",
   "metadata": {},
   "source": [
    "#### Network In Network\n",
    "* the main idea is to put a network inside another network\n",
    "\n",
    "* they introduced multi layer preceptron conv layer which is a conv layer followed by a few FC layers\n",
    "* this idea is bisacally a (one to one convution) \n",
    "* they introduced a global averaging pooling: insted of adding a bunch of FC layers at the end of teh conv architecture, we can just average multible channels  from the last conv layer to form the output layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0739a3",
   "metadata": {},
   "source": [
    "* one by one convolution is a normal convolution with fliter size of 1 by 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781b1273",
   "metadata": {},
   "source": [
    "* in conv net, we want the network to be invariant both localy and globaly, which means we still predict the photo is for a dog, even if the dog had  slight shift in pixels (local invariant), and also of the dog went to be in the lower corner of the pic isntead of the upper one (global invariant)\n",
    "* we can achieve local invariant with pooling, and deal with global invariant with data augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52a5aec",
   "metadata": {},
   "source": [
    "\n",
    "#### VGG Net\n",
    "\n",
    "##### Local Response Normalization:\n",
    "* the idea is to normalize a pixel across nearing channels \n",
    "\n",
    "* after comparing nets with lrn and nets without, they didn't find big difference, so they stoped using it \n",
    "\n",
    "##### Data Augmentation\n",
    "* Image translations( random crops), and horizontal reflection \n",
    "* altering the intensities of the RGB channels \n",
    "* scale jittering   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c030d81",
   "metadata": {},
   "source": [
    "#### GoogleNet\n",
    "\n",
    "* You stack multiple inception modules on top of each ohter \n",
    "* the idea is that you don't have to choose which filter size to use, so why don't use them all \n",
    "* to make the network more efficient, they first projected the input with one by one convolution then applied the main filters \n",
    "* you concatinate the many filters through the channel dimension    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab15387",
   "metadata": {},
   "source": [
    "#### Batch Normalization \n",
    "\n",
    "* The main goal of batch normalization is to redude the `Internal Covariant Shift`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f025de2",
   "metadata": {},
   "source": [
    "* we can just normalize the inputs and it would work fine\n",
    "* the problem is that in each following layer, and statistics of its output would depend on its weights \n",
    "* so we also need to nomalize the inputs in hidden layers \n",
    "* here, the gradient is also going through the mean and variance operations , so it gets a snese of whats gonna happen\n",
    "\n",
    "* in inference we can't have batch-dependant mean and variance, so we use the average mean and variance for the whole dataset \n",
    "\n",
    "##### conv layers\n",
    "* for conv layers we apply normalization across every channel for every pixel in the batch of images\n",
    "* the effective bach size would be ==> m*p*q where m is the number of images in the batch and \n",
    "p,q are the image resolution \n",
    "\n",
    "##### Benifits of batch norm:\n",
    "* you can use higher learning rate, as the training is more stable \n",
    "* less sensitive to initialization \n",
    "* less sensitive to activation function \n",
    "* it has regularization effects, because thre's random mini batch every time \n",
    "* preserve gradient magintude ?? maybe --> because the jacobian doesn't scale as we scales the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a127c4",
   "metadata": {},
   "source": [
    "#### Parametric Relu:\n",
    "\n",
    "$ f({y_i}) = \\max(0,y_i) + a_i \\min(0, y_i) $\n",
    "* if $a_i = 0$  --> Relu\n",
    "* if $a_i = 0.01$ --> Leaky Relu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bceafe0",
   "metadata": {},
   "source": [
    "* the initialization of weights and biases depends on the type of activation function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09ceec6",
   "metadata": {},
   "source": [
    "#### Kaiming Initialization  (I didn't fully understand the heavy math in this lecture, as Im still weak in statistics and variance calculations):\n",
    "\n",
    "- professor went into deep mathematical details into how to choose the intial values for weights\n",
    "* the main idea is to investigate the variance of the response in each layer, so we start by calculating the variance for the output of the layer, and we end up with many terms of the weights multiplied together, so to prevent it it from vanishing or exploding, we  want the weights to have values centred around 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127625d2",
   "metadata": {},
   "source": [
    "#### Label smoothing regularization\n",
    "- the idea is to reagularize the notwork by giving random false labels for a few examples of the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3f5b57",
   "metadata": {},
   "source": [
    "#### ResNet \n",
    "\n",
    "* The main idea is to make the NN deeper so that it becomes better, but the idea is that when you do that, the network gets worse, so we can fix that by adding a resdual connection.\n",
    "\n",
    "##### Identity mapping in resnets \n",
    "\n",
    "* the idea is to do no non-linear operations on the main branch(identity mapping), so that the keep a deep flow of the data both in forward and backward pathes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e03f0d3",
   "metadata": {},
   "source": [
    "#### Wide Residual Networks \n",
    "\n",
    "* an attempt to make resnets wider and study if that would make them better "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf26fea",
   "metadata": {},
   "source": [
    "#### ResNext \n",
    "* just like resnets but they changed bottleneck blocks with group convolution block "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ebea09",
   "metadata": {},
   "source": [
    "#### Squeeze-and-Ecxcitation Networks \n",
    "\n",
    "##### Squeeze : just a global averaging step \n",
    "##### Excitation: is just a fully connected newtwork \n",
    "##### Scaling : multiply every channel with the corresponding exctitiaiton value, more like attention \n",
    "* scaling is you paying different attention to different channels like attention models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04e8e1",
   "metadata": {},
   "source": [
    "#### Spatial Transformer Network \n",
    "* the main idea is to seperate the main object in the image, like putting a box around it and then this box can be resized, shifted, rotated. so in the end we have a focused image that has only the object, and so we can apply convolution on it and it would be easy then \n",
    "\n",
    "* the idea is to first find a good transformation parameters theta, you can do that using NN\n",
    "* then for every position in the output image, you do a bilinear sampling from the input image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82973f30",
   "metadata": {},
   "source": [
    "#### Dynamic Routing between capsuls \n",
    "\n",
    "* the idea is to make the outputs of the capsule has a norm that is the probability that an object is presenet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09096219",
   "metadata": {},
   "source": [
    "### Small Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae43e15",
   "metadata": {},
   "source": [
    "#### Knowledge Distillation\n",
    "\n",
    "* the main idea in to use an artificial data coming from the gaint model, using the normal training dataset and a smoothed the output from the giant model. then we train the distilled model using this dataset and with the same parameter `T` that we used to smooth the data. then in production we set the temperature parameter to 1 and use the distilled model for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba8bbb",
   "metadata": {},
   "source": [
    "#### Network Pruning:\n",
    "*  all connections with weights below a threshold are removed from the network \n",
    "* weight are sparse now \n",
    "* then we can represent them using fewer bits\n",
    "\n",
    "#### Quantization\n",
    "* we basically cluster our weight to some centroids\n",
    "* the number of centroids for conv layers are more than the ones for FC layers why:\n",
    "    -   because conv layer filters are already sparse, we need higher level of accuracy in them\n",
    "    -   FC layers are so dense that we can tolerate fewer quantization levels \n",
    "\n",
    "#### Huffman Coding\n",
    "\n",
    "* store the more common symbols with more bits "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44692d3a",
   "metadata": {},
   "source": [
    "####  Squeeze Net\n",
    "* the idea is to squeeze the network by using one by one convolution thus use one smaller firlter sizes, then expand to make up for the squeeze that is made \n",
    "* the main idea  is to use one by one comvultion to reduce the dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c55aa",
   "metadata": {},
   "source": [
    "#### XNOR-NET\n",
    "\n",
    "* the idea to to convert the weights and inputs to binary values, and so we save a lot in memory and computation\n",
    "* the idea is to use a pre trained weights, then you try to binariez the weights by trying to approximate ==> $W = \\alpha * B $ where alpha  is postative 32 bit constant and B is a binary matrix \n",
    "* then mean we try to train by using a means square error loss function of the original weights and alpha and B \n",
    "\n",
    "* I still can't fully understand  how to binarize the input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c248f32e",
   "metadata": {},
   "source": [
    "#### Mobile Nets\n",
    "\n",
    "* the idea is to reduce computation complexity by doing c   onv for each channel separately, and not across channels.\n",
    "* so we use number of filters as the same as the input channels\n",
    "* but then we will end up with  output size as the input size, so we still need to do one by one convolution to output the correct size\n",
    "\n",
    "#### Xception\n",
    "* unify the filters sizes for the inception, and then apply them for each channel separately, then do one by one convolution to fix the output size\n",
    "\n",
    "#### Mobile Net V2\n",
    "* the same as MobileNet, but with Residuals connections.\n",
    "\n",
    "#### ShuffleNet\n",
    "\n",
    "* the idea is to shuffle channels after doing a group convolution "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6114d034",
   "metadata": {},
   "source": [
    "### Auto ML\n",
    "\n",
    "* the question is can we automate architicture engineering, as we automated feature engineering in DL?\n",
    "* we can use RNN to output a probability, to sample an architicture from, then use train using this arch, and give the eval acc, as a feedback to the RNN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf3caa",
   "metadata": {},
   "source": [
    "\n",
    "#### Regularized Evolution\n",
    "* it's basically random search + selection\n",
    "* at first you randomly choose some  architecture  train, and eval on it and push it to to the population\n",
    "* then you sample some arch. from the population\n",
    "* then u select the best acc model from your samples , and then mutate it (ie. change some of its arch.), then add it to your samples \n",
    "* then remove the oldest arch. in the population\n",
    "* you keep repeating this cycle till you evolve for C cycles (history size reaches the limit) and report the best arch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cd4f8d",
   "metadata": {},
   "source": [
    "#### EfficientNet \n",
    "\n",
    "* the idea is that we do grid seach on a small network to come with the best depth scaling coefficient `d`, width scaling coefficient `w`, and resolution scalling coefficient `r`, then we try to find scaling parameter $\\phi$, that gives the best accuracy while maintaning the `flops` under the limit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aedb8a",
   "metadata": {},
   "source": [
    "### Robustness \n",
    "\n",
    "* The main goal is to make your network robust against adverarial attacks\n",
    "\n",
    "#### Intrigiong peroperties of neural networks \n",
    "* there's nothing special about individual units, and the individual features that the network learn, and they you can interpret any random direction. So, the entire spacd matters \n",
    "* neural networks  has blind spots, this  means you can add small pertirbations to an image, they are not noticable to the human eye, but they make the network wrongly classify the image  \n",
    "* Adversiral examples tend to stay hard even for models trained with different hyper-parameters, or ever for different training datasets \n",
    "\n",
    "* you can train your network to defend against attacks but that's expensive, as: first, you have to train your network, then train it again to find some adversiral attacks, then add those examples to the training set, and finally train for a third time.\n",
    "\n",
    "* small perturbation to the image, leads to huge perturbation to the activation, due to high dimensionality\n",
    "#### untargeted adversiral examples\n",
    "* fast gradient sign: using the trick of the sign of the loss gradient, and add it to the original image to generate an adversiral example \n",
    "* then you can just add a weighted loss, one for the  orginal example, and another for the adversiral one, so that the network would be more robust to adversiral examples \n",
    "\n",
    "#### Towards Evaluating the Robustness of Neural Networks\n",
    "* another way to generate targetted adversiral examples is: to choose a function that forces the network to make the logits for the targeted example the biggest, so that this class is selected. \n",
    "\n",
    "![](assets/Applied_deep_learning/adversiral-attack-algo.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa88c77",
   "metadata": {},
   "source": [
    "### Visualizing & Understanding\n",
    "* now we want to debug our network, to understand how it works\n",
    "* so we want do a backward pass, by inverting our forward pass, but then we habe a problem with pooling layers as we subsamples the input.\n",
    "* so we store the locations for the max pixels that we choose in our pooling operation, so that we can upsample the input again in the backward pass.\n",
    "* we call these max locations, switches\n",
    "* the main idea is, visualising the feature maps, gonna help you modify the network \n",
    "* you can have two models that have the same output for the same input but which one do you trust more?\n",
    "    - to answer that, you need to see which features each one of them focuses on, so if one of them focuses on features that are important to classfication, then this model is more trustworthy \n",
    "#### LIME: Local Interpretable Model-agnsortic Explanations\n",
    "* you want to trust the model, meaning that you wanna make sure the model prioritized the important features\n",
    "* but you can't interpret non linear models, so the idea is to make a locally linear model, that have the same output for your local input example, then use this linear model to get the features that the model prioritized \n",
    "\n",
    "#### Understanding Deep Learning Requires Rethinking Generalization\n",
    "* NN are powerful enough to fit random data, but then it will not generalize for test data \n",
    "* so when we introduce radom labels, random pixels, etc: we still can go for 0 train loss, but for test data, the error is gonna be equal to random selection.\n",
    "* so, this means: The model architecture itself isn't a sufficient regularizer.\n",
    "\n",
    "* Explicit regularization: dropout, weight decay, data augmentation\n",
    "* Implicit regularization: early stopping\n",
    "* there exist a two-layer NN with Relu activation, that can fit any N random example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09f6e37",
   "metadata": {},
   "source": [
    "### Transfer Learning\n",
    "\n",
    "* labled data is expensive\n",
    "* you split a data set in half,we find that transfer learning for the same task, have higher acc'\n",
    "* transfer learning with fine-tuned weight is better than locking the learned weights\n",
    "* on average you just wanna cut the network in he middle and start learingn after few layers, as the first few layers ar more general leayers and can acctually help you in traninge for another task\n",
    "#### DeCAF\n",
    "* first layers learn low-level features, whereas latter layers learn semantic or high-lebel features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2867f7ff",
   "metadata": {},
   "source": [
    "## Image Transformation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a6427b",
   "metadata": {},
   "source": [
    "### Semantic Segmentation\n",
    "* you want to segments different classes in the image\n",
    "* The fully connected layers can also be viewed as convoluting with kernels that cover their entire input regions.\n",
    "\n",
    "#### Atrous Convolution:\n",
    "* you don't wanna lose much info when you do conv,  and then upsample again, so you fill your filter with holes, so that you lose less info\n",
    "* reduce the degree of signal downsampling \n",
    "#### CRF: \n",
    "* deals with the reduced localization accuracy due to the Deep Convolution NN invariance \n",
    "\n",
    "#### Dilated Convolution:\n",
    "* basically atrous convolution\n",
    "* increases the  size of the receptive points layer by layer\n",
    "\n",
    "### Image Super-Resolution \n",
    "* we want to develope a NN that can up-sample images\n",
    "* we can do that using convolution \n",
    "* and in the middle we use one to one convolution to work as non-leaner mapping\n",
    "\n",
    "### Perceptual Losses \n",
    "* mse isn't the best for images, for example, if we shift an image by one pixel in any direction, we will end up with huge loss, while the two images are the same \n",
    "* the idea it to use a CNN like VGG-16 to calculate the loss, this works because any CNN would have some perceptual understanding of the images\n",
    "* so we push the output of our model, and the target (label) through a NN, and compare the feature maps on different layers\n",
    "#### Single Image Super-Resolution(SISR)\n",
    "* the idea is to make the network to only learn the residual not the full image, so it just learns the difference between the two images \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3211165",
   "metadata": {},
   "source": [
    "## Object Detection\n",
    "\n",
    "### Two Stage Detectors \n",
    "\n",
    "#### R-CNN\n",
    "* we enter the input image into extract regions algorithm. this algorithm is cheap algorithm that output millions of boxes per image. we do that using an algorithm called \"selective search\"\n",
    "* we then enter that to a CNN to do features extractions\n",
    "* at the end we have a per-class classifier \n",
    "\n",
    "#### Spatial Pyramid Pooling \n",
    "* the idea is that we use spatial pyramid pooling to have a fixed length representation for the image\n",
    "* also we push the input image once through the conv layers, then choose multiple windows after to do the classification for. this way we cut so much on computations cause we for the first few conv layers, we pushed just one image\n",
    "\n",
    "#### Fast R-CNN\n",
    "* just like RCNN but, changed the multi-class SVM with multi-task loss, this way we don't have to calculate many classifiers, one for each class.\n",
    "* also we don't need bounding box proposals, and we can acc train a Region Proposal Network, to propose bounding boxes for every pixel in the feature map.\n",
    "* last trick is to use a CNN instead of the FC head at the end of the network, but CNN is translation invariant, so we need to do pooling for each region separately. \n",
    "#### Feature Pyramids\n",
    "* the idea is that we need to use different versions for our input image, each with different resolution, so that we detect objects with different sizes.\n",
    "* to do that we can use the different features maps at different layers, so that at each layers the resolution changes, and we can use that to choose our windows\n",
    "* the problem is that each layer represent a different semantic meaning of the image, so the first few layers consider the image colors, while the last few consider the more complex shapes of the image\n",
    "* to overcome this, from each layer we add a connection to the layer below\n",
    "* so we up sample the feature maps first then do one by one convolution to adjust the number of channels,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1892e1f",
   "metadata": {},
   "source": [
    "### One Stage Detectors \n",
    "#### YOLO \n",
    "* we want the detector to be realtime, so we can detect objects live\n",
    "* divide the input image into  S * S grid\n",
    "* if the center of an object fell inside a cell, that cell is the one responsible to detect that object \n",
    "* each grid cell gonna predict, B bounding boxes, each with confidence score\n",
    "\n",
    "#### SSD: Single Shot MultiBox Detector \n",
    "* we want to take the speed from YOLO, and the high acc from the two-stage detectors\n",
    "* unlike YOLO, we can use early layers, not just the last layers of the network, and for each one we can predict more boxes, so we end with much more boxes than YOLO\n",
    "#### YOLO9000 - YOLO V2\n",
    "* Tries to improve upon YOLOv1 using idead from fast-CNN and SSD \n",
    "* we can use higher res images in training\n",
    "* can the anchor boxes from the training images not just randomly \n",
    "* introduced passthrough layer\n",
    "* used hierarchical classification to extend many classes \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630c6669",
   "metadata": {},
   "source": [
    "## Video\n",
    "### Large-scale video classification \n",
    "* we would have a context stream which learns features on a low res frames.\n",
    "* and a Fovea Stream, which operates on a high res middle portion of the frames \n",
    "\n",
    "#### Early Fusion\n",
    "* the idea is to take a few frames from the middle of the clip, and apply conv on them, the only diff is that we add a new dimension to filters which coreespond to the number of frames \n",
    "* that's just for the first layer, but then it's normal conv\n",
    "#### Late Fusion\n",
    "* we have two separate single-frame networks, each one takes a diff frame from the clip, and we concatenate them in the end \n",
    "\n",
    "### Two-Stream CNN for action recognition \n",
    "* video can be decomposed into spatial and temporal components \n",
    "\n",
    "#### Optical flow stacking\n",
    "* we can just follow pixels from frame to another, and then create a flow vectors, in the x,y axis \n",
    "* then we can stack these flow vectors \n",
    "#### Trajectory stacking\n",
    "* follow the point from frame to another \n",
    "\n",
    "### Non-local Neural Network\n",
    "* the idea is that we want to see for output pixel, which areas did it pay attention to in the input\n",
    "* so we attention every output with all possible pixels in the input\n",
    "* if we are using it with videos, then we add another dimension for the time\n",
    "\n",
    "### Group Normalization \n",
    "* Batch norm, is good as long as we have reasonable batch size\n",
    "* but whe we have very small batch size, then batch norm isn't the best \n",
    "\n",
    "**Here's diff between normalizaiton methods:**\n",
    "\n",
    "\n",
    "![](assets/nomalizations.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f30a3",
   "metadata": {},
   "source": [
    "# Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1a4860",
   "metadata": {},
   "source": [
    "## Word Representation \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79531a0",
   "metadata": {},
   "source": [
    "### Distributed Representation of Words and Phrases and their Compositionality\n",
    "\n",
    "#### Word2Vec ( Efficient Estimation of Word Representation in Vector Space )\n",
    "* using CBOW model, or skip-gram model\n",
    "* uses the cosine-similarity distance function \n",
    "\n",
    "##### Skip-gram Model\n",
    "* Continuous Bag of Words is the opposite of the skip-gram in the sense of what are we predicting (word vs context)\n",
    "\n",
    "* the idea is that you pick a word, and try to predict the context around it\n",
    "* so you have a word in the middle and try to predict words around in (before, and after), given a defined window size\n",
    "* and our objective is to maximize the liklihood of the context given the reference word \n",
    "* we can use binary trees, to do an approximation, and speed up the softmax caculation, as for every word in the vocab, we would calculate it's softmax with all other words,but now we can use binary trees, and do that in just log(n), using an approximation, that we group words together, and in each level coming from the root, we go right or left, till we reach the word in the leaves \n",
    "* we can make this even faster, using huffman encoding to assign shorter paths for more frequent words \n",
    "\n",
    "* noise sampling: the idea is to give the model negative samples, that doesn't appear together, and give it low probability \n",
    "\n",
    "###### Evaluation \n",
    "* we can evaluate the model, using syntactic, and semantic analogies \n",
    "* for example, Berlin to Germany is like France to Paris\n",
    "\n",
    "#### GloVe: Global Vectors for Word Representation \n",
    "* the idea is to use:\n",
    "    - global matrix factorization methods\n",
    "    - local context window methods \n",
    "* we compute a co-occurrence method, that holds the counts every two words come after each other, we try to learn two matrieces and two biases, that log(X) = w1 * w2 + b1 + b2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de18980",
   "metadata": {},
   "source": [
    "## Text Classification \n",
    "\n",
    "### Recursive deep models for semantic compositionality over sentiment Treebank\n",
    "* the dataset is presented as a tree with leafs as words\n",
    "* the idea is that for evert word we get it's embedding, then we multiply that by a weight matrix, and apply softmax, so then we have probability distribution over our classes (sentiment classes)\n",
    "* then we can concatenate every words together, and keep recursing till we finish the whole sentence \n",
    "* the problem with this model, is that we are losing the pair wise interaction between the two words,\n",
    "* wat we can do it introduce a tensor V, that would capture this interaction  \n",
    "### CNN for Text Classification \n",
    "* we want to use CNNs with text, so we would have some filters\n",
    "* the idea is to treat sentences as one dimensional vector, and then we can apply windows that contain bunch of words to some filters, and aggregate them \n",
    "\n",
    "## Doc2Vec\n",
    "\n",
    "* as we have representation of words, we can also have the same for sentences, or documents\n",
    "### Bag of words\n",
    "* for each sentence, count the frequency of each word in your vocab\n",
    "#### weakness\n",
    "* lose ordering or words\n",
    "* lose semantics or words \n",
    "    - \"powerful\" should be closer to \"strong\" than \"Paris\" \n",
    "\n",
    "### Paragraph vector\n",
    "* for every paragraph, we would have a vector representing it, then we can average those together, and try to get the target paragraph\n",
    "* we can do it as CBOW, and instead of words, we would have paragraphs \n",
    "\n",
    "\n",
    "## FastText \n",
    "* the idea is that we take a sentence(a bag) of words, or N-grams and then sum their vectors together, then project them to latent space, and then project them again to output space, and apply non-linearity (softmax for example), then apply cross-entropy as a loss function \n",
    "* we can also normalize our bag of features (the word representation), so we down weight most frequent words \n",
    "* instead of softmax, we can use hierarchial softmax, to decrease the training time\n",
    "* this model is super fast, and gives results similar to non-linear complicated models like CNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57705ac",
   "metadata": {},
   "source": [
    "## Hierarchial Attention Networks for Document Classification \n",
    "* the idea is that we want to do document classification\n",
    "* so in the end we want to represent the document by a vector, that we can enter to softmax, and then output a class \n",
    "* we can use think of document as they are formed of sentences, and sentences are formed of words \n",
    "* so we can use GRU based sequence encoder to represent words and then sentences \n",
    "\n",
    "\n",
    "### GRU \n",
    "* we have a reset gate, and update gate\n",
    "* the idea is that at every step we have a previous hidden output, and a current input\n",
    "* then we have an update gate that determines  the percentage to take from the current hidden output, vs the previous hidden output.\n",
    "* then we also have a reset gate, which determines how much we wanna take from the previous state when calculating the current state \n",
    "* we use the tanh function to calculate the hidden state\n",
    "* we use the sigmoid to calc the parameter Z, which tell us the percentage between the current state, and the previous state output\n",
    "\n",
    "* we can have a forward, and a backward GRU, and concat them \n",
    "* then we project these concat words representation, and apply non-linearity \n",
    "* then to calculate the sentence vector out of these word vectors, we apply a weighted average on them.\n",
    "* this works like a simplified version of attention \n",
    "* we can do this weighted average using softmax, but first we need to turn this vector to a scaler, which we can do by  applying dot product with \"query\" or \"word context\", like we are doing a query: what is the informative word \n",
    "* this will get us with the alphas, which tell us how much we take from each word vector \n",
    "\n",
    "* NOTE: cross-entropy with one-hot vector is the same as the log-liklihood "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867b25f8",
   "metadata": {},
   "source": [
    "## Neural Architecture for Named Entity Recognition \n",
    "### LSTM-CRF Model\n",
    "#### Normal LSTM\n",
    "*  has 3 gates, input gage, forget gate, output gate.\n",
    "#### Conditional Random Field \n",
    "* in NER, the named tags are dependant on each other, for example: `B` tag, and `I` tag.\n",
    "* so we want to account for that in our loss\n",
    "* to do that we introduce a new compatibility matrix, to count for this dependency \n",
    "#### Character-based models of words\n",
    "* we need it cause, in production, we might encounter new unseen words, so we make up for that using the character encoding \n",
    "* we want to add character representation with our words representation\n",
    "* so we do a character-based bi-LSTM\n",
    "* and we concatenate the output of the LSTM, with our word representation from the lookup table \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f0cc68",
   "metadata": {},
   "source": [
    "## Universal Language Model fine-tuning for Text Classification (ULMFiT)\n",
    "* Language Model: a model that trying to give an understanding for language. Like given few words of the sentence, can we guess the next word. \n",
    "### Disctimonative fine-tuning \n",
    "* so the idea is to split the model pre-trained parameters for each layer\n",
    "* and to also choose a learning rate for each layer\n",
    "* the early layers would have smaller learning rate, so their weight wouldn't update as much\n",
    "\n",
    "### slanted triangular learning rate \n",
    "* the idea is just to increase the LR gradually, till some point, then decrease in again\n",
    "* and we do the increase and decrease linearly, so we end up with the triangular shape \n",
    "### gradual unfreezing \n",
    "* gradually unfreezing parameters through time \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d191d7b",
   "metadata": {},
   "source": [
    "## Natural Machine Translation bu Jointly Learning to Align and Translate \n",
    "* we wanna model the conditional probability $ p(y|x)$. where x is the source sentence, and y is the target sentence \n",
    "\n",
    "### RNN Encoder-Decoder \n",
    "#### Encoder\n",
    "* the encoder gonna encode our entire sentence into a single vector c \n",
    "* we can use LSTM, which will output a sequence of vectors $ h_1, h_2, \\dots ,h_T$. we can choose c to be just the last vector of the LSTM $h_T$\n",
    "### Decoder\n",
    "* we can model $p(y|x)$, as that product of $y_i$ for i from 0 to input time T.\n",
    "* but we can do an approximation, that instead of X, we calc using C which is a representation of X.\n",
    "* and instead of using the previous Y outputs in previous time steps, we can use the previous hidden state\n",
    "\n",
    "![](assets/rnn_encoder_decoder.png)\n",
    "\n",
    "### BLUE Score ( Bilingual Evaluation Understudy )\n",
    "* [Good reference](https://www.youtube.com/watch?v=DejHQYAGb7Q)\n",
    "* provides an automatic score for machine translation \n",
    "* the normal precision gives terrible results\n",
    "* they introduced a modified precision score, which gives score to words up to the maximum number of occurrences in the reference sentences \n",
    "* we need also to account for different grams.\n",
    "* for example, for bi-grams, we would count the bi-grams in the output, and count-clip them at the maximum of the bi-gram in the reference sentences\n",
    "* Pn = BLUE score on n-grams only \n",
    "* Combined Blue score:   Bp exp(1/n * sigma(Pn)) \n",
    "    - Bp: brevity Penalty\n",
    "    - it basically penalize short translations \n",
    "    - Bp: is one  if output  is longer than reference \n",
    "    - otherwise, it's exp(1 -  (output length / reference length))\n",
    "## Sequence to Sequence Learning with Neural Networks\n",
    "* One limitation of RNNs is that the output sequence, is the same length as the input sequence  \n",
    "* this is using two different LSTMs  one for input, and one for output \n",
    "* it stacks multiple LSTMs together creating deep LSTM\n",
    "* reversing the order of the words of the input sentence \n",
    "    - the intuition is that the first of the output is gonna take most info from the first tokens of the input \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cea064e",
   "metadata": {},
   "source": [
    "## Phrase representation \n",
    "* they combined DL approaches like RNNs, with statistical ML approached to enhance the translation\n",
    "* we cen learn word embedding from the translation task\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82d929",
   "metadata": {},
   "source": [
    "## Attention-based Neural Machine Translation \n",
    "* attention solves the problem of decreasing Blue-score with increasing the sentence length \n",
    "### Global Attention \n",
    "* with previous approaches, we used a small version of attention, to choose which source vector would have the bigger weight\n",
    "* in this version we do the same, but with different, source-target hidden state vectors\n",
    "* so we attend source and target hidden state vectors \n",
    "<center> <img src=\"assets/global-attention-model.png\"/></center>\n",
    "\n",
    "### Local Attention\n",
    "* instead of attending to all of the input space, we can jus attend to a portion of it \n",
    "* it's faster than global attention\n",
    "* how to choose the portion to attend to, is learnable \n",
    "<center> <img src=\"assets/local-attention.png\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36326f1",
   "metadata": {},
   "source": [
    "## Byte Pair Encoding \n",
    "\n",
    "* the main objective is to reduce the amount of unknown words\n",
    "* the idea is to iterate over the bytes in the sequence and pick the most frequent one and replace it with an unused byte\n",
    "* a byte is a character, or a sequence of characters \n",
    "* we keep doing that till we convert our input sequence to bunch of bytes\n",
    "* at test time, we would have Out Of Vocab words, but we can convert them to known bytes that we extracted in training. \n",
    "* we are trying to find a balance between word-encoding and character-encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4a645e",
   "metadata": {},
   "source": [
    "## Google's Neural Machine Translation \n",
    "* the first paper to beat statistics MT methods \n",
    "* we will use encoder RNN to encode the input\n",
    "* then we will use attention, to attend to the input\n",
    "* they added 8-layer LSTM with residual connection \n",
    "* they used (Byte-Pair) word-pieces technique\n",
    "* the loss function, is the log likelihood of the output, conditioned on the input, but we wanna to add the GLeu score to penalize depending on the quality of the translation, so we can add the Gleu score to the loss function as a reward, in RL\n",
    "* the did beam-search which penalized small sentences, and added penalty for long sequence contexts \n",
    "* lastly, they quantize the model, and its parameters in inference  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594d034",
   "metadata": {},
   "source": [
    "## Convolution Sequence to Sequence Learning \n",
    "* we want to use the parallelization of the CNNs to learn seq-to-seq \n",
    "* we add positional embedding to account for the different sentence positions. we didn't need to do this for RNNs as they process words sequentially by default \n",
    "* The network has and encoder, and a decoder\n",
    "    - the encoder, process all input \n",
    "    - the decoder, only considers the previous inputs\n",
    "* we have a stack of conv blocks\n",
    "* for each convolution block, you take a k words, each is d dimensional, then you flatten them to be (kd) dimensional, and apply convolution, which is multiplying by filter of size (2d,kd), then you have an output of (2d) dimension. you talk the first half and dot product it with the sigmoid of the second half. there we applied the non-linearity.\n",
    "* and for each block, we also add residual connection.\n",
    "* lastly, we add attention between our encoder blocks output, and the decoder blocks output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f43760e",
   "metadata": {},
   "source": [
    "## Attention Is All You Need\n",
    "* in RNNs and CNNs, there's this inductive bias, that the useful information, is right next to us. while, is Attention we don't assume that \n",
    "* Just as all attention based models, we need to add positional encoding\n",
    "    - they do that by fourier expansion \n",
    "* all previous work was cross attention between encoder, and decoder\n",
    "* here they introduced, self-attention, where the encoder attend to its inputs \n",
    "* Then the added residual connection and layer normalization\n",
    "\n",
    "### One Head Attention \n",
    "* we have Query, Key, and Value.\n",
    "    - we multiply each of them with a weight matrix to add learnable parameters\n",
    "* first, we do attention between, the query and the key, they we down-scale the dot product by the square root of the embedding dimension.\n",
    "    - we choose the square root, because it's not big nor small number, so we keep the attention weights in a reasonable range\n",
    "* then we do a weighted sum between the attention weights and the Value matrix \n",
    "### Multi Head Attention\n",
    "* then one the most important ideas here is multi-head attention\n",
    "* we make many single head attention, then concatenate them together\n",
    "\n",
    "### Decoder\n",
    "* the same as the encoder\n",
    "* the main difference, is that we do masked attention, which only attend to previous outputs only \n",
    "* Here, the query is coming from the output sequence, and the key, and value, are coming from the encoder \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fbe742",
   "metadata": {},
   "source": [
    "## Subword Regularization \n",
    "\n",
    "* we want to have my many Subword tokenization for the same word\n",
    "* the multiple subword tokenization works like kind of data augmentation, and also adds a regularization effect "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4feeb77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
