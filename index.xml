<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Boda Blog</title><link>https://bodasadalla98.github.io/blog/</link><description>This is my personal Blog, I write about random things that comes to my mind!</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bodasadallah@gmail.com (Boda Sadallah)</managingEditor><webMaster>bodasadallah@gmail.com (Boda Sadallah)</webMaster><lastBuildDate>Mon, 23 May 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://bodasadalla98.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>Programming Facts</title><link>https://bodasadalla98.github.io/blog/programming_facts/</link><pubDate>Mon, 23 May 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/programming_facts/</guid><description><![CDATA[Intro this is intended to be some kind or reference to go through whenever I face some kind of bug, or error, that I don&rsquo;t know how to solve.usually these kinds of error that doesn&rsquo;t make sense, or we don&rsquo;t know the cause of them, and the worst it, that we don&rsquo;t find many ppl facing the same issue, so internet can&rsquo;t be so useful then. I learned theses facts the hard way, spending so much time trying to figure out the root of the issue.]]></description></item><item><title>Deep Learning Papers Summarization</title><link>https://bodasadalla98.github.io/blog/papers/</link><pubDate>Fri, 22 Apr 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/papers/</guid><description>Decoupled Neural Interfaces using Synthetic Gradients In NN, the training process, has 3 bottle-necks forward lock: you need to calculate teh output of the previous layer before you can can go into next layer in forward pass backward pass: the same, but for backward propagation weights lock: you can&amp;rsquo;t update weights unless you do for weights in next layer the paper trying to unlock these bootle-necks by decoupling each layer, to be sufficient alone it does that by introducing, a Synthetic Gradient Model, that can predict the gradient for the current layer, without waiting for the gradient of the next layer this was we can calculate gradient and update weights as soon as we calculate the activation of the current layer Synthetic Gradient Model can be just a simple NN that is trained to output the gradient of the layer</description></item><item><title>GIT</title><link>https://bodasadalla98.github.io/blog/git/</link><pubDate>Tue, 15 Mar 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/git/</guid><description>Beatiful commands git log --oneline --decorate --all --graph
git merge --abort ==&amp;gt; abort merge, and get back like it never happened
git reset --hard ==&amp;gt; is your way to lose all uncommited work in your working directory
git fast forward is basically that git moves the commit pointer upward to the new posotion, without creating a merge commit or anything you can merge with --no-ff flag, to disable the fast forward merge and force git to create the merge commit Git Bisect used when something broke, and you know what did broke, but you can&amp;rsquo;t figure out when did it broke you just give it a testing criteria to test the commit history against Methodology everything inside git is an object all your local branches are located in .</description></item><item><title>Mind Exercise</title><link>https://bodasadalla98.github.io/blog/mind_exercise/</link><pubDate>Sun, 13 Mar 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/mind_exercise/</guid><description><![CDATA[Sad Thoughts I think this would be the first post, of random thoughts than cross my mind. Some of these thoughts are sad, and some are with good vibes, but the sad ones are more to be honest. I don&rsquo;t know why, that&rsquo;s the case, maybe cause lately for the past few years I&rsquo;ve been feeling lonely. It&rsquo;s becoming a constant feeling, that would just lurks in the shadows while im busy with something, but it wouldn&rsquo;t hesitate to jump out as it gets the chance.]]></description></item><item><title>Stanford CS224N NLP with Deep Learning</title><link>https://bodasadalla98.github.io/blog/stanford_nlp_cs224n/</link><pubDate>Tue, 08 Mar 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/stanford_nlp_cs224n/</guid><description>Post for Stanford NLP Course</description></item><item><title>Applied Deep Learning [Course](https://github.com/maziarraissi/Applied-Deep-Learning)</title><link>https://bodasadalla98.github.io/blog/applied_deep_learning/</link><pubDate>Tue, 01 Feb 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/applied_deep_learning/</guid><description>Reference Repo with the slides, and course info
Deep Learning overview we can look at deep learning as an algorithm that writes algorithms, like a compiler in this case the source code would be the data: (examples/experiences) excutable code would be the deployable model Deep: Functions compositions $ fl f{l-1} &amp;hellip;. f_1$
Learning: Loss, Back-propagation, and Gradient Descent
$ L(\theta) \approx J(\theta)$ &amp;ndash;&amp;gt; noisy estimate of the objective function due to mini-batching.</description></item><item><title>NLP Specialization</title><link>https://bodasadalla98.github.io/blog/coursera_nlp_specialization/</link><pubDate>Sun, 30 Jan 2022 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/coursera_nlp_specialization/</guid><description>Course 1: Classification and vector Spaces Weak 4 Hashing We can use hashing to search for the K-nearest vectors, to heavily reduce the searching space Locality senstive hashing the idea is to put items that are close in the vector space, in the same hashing buckets
we can create a set of planes and calculate the relative position of points compated to this plane and then we can calculate the hash value for this point accordingly</description></item><item><title>CS480/680 Intro to Machine Learning</title><link>https://bodasadalla98.github.io/blog/introduction_to_machine_learning/</link><pubDate>Tue, 14 Dec 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/introduction_to_machine_learning/</guid><description>Lecture 12 Gausain process infinite dimentional gaussian distribution Lecture 16 Convolution NN a rule of thumb: to have many layers with smaller filters is better than having one big filter, as going deep captures better features and also uses fewer parameters Residual Networks even after using Relu, NN can still suffer from gradient vanishing the idea in to add skip connections so that we can create shorter paths Lecture 18 LSTM vs GRU vs Attention LSTM: 3 gates, one for the cell state, one for the input, one for the output</description></item><item><title>Voice Conversion</title><link>https://bodasadalla98.github.io/blog/voice_conversion/</link><pubDate>Sun, 21 Nov 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/voice_conversion/</guid><description> AutoVC paper Demo Repo Speach Split Paper Demo repo AutoPST Paper Demo repo Sources IMPROVING ZERO-SHOT VOICE STYLE TRANSFER VIA DISENTANGLED REPRESENTATION LEARNING Voice Conversion Challenge 2020 results</description></item><item><title>Python</title><link>https://bodasadalla98.github.io/blog/python/</link><pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/python/</guid><description>Inheritance If you have inherited from parent class then you should call the parent class constructor if you overload it, or simply doesn&amp;rsquo;t overload it Ex:
1 2 3 4 5 6 7 8 9 10 class parent: def __init__: class child: def __init__: super().__init__() class child2: ## or simply don&amp;#39;t override the constructor and use the parent one Multiple Inheritance when we inherit from two or more classes, whatever class we inherited first(typed first in the list), would be the one to have pariority</description></item></channel></rss>