<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Boda Blog</title><link>https://bodasadalla98.github.io/blog/</link><description>This is my personal Blog, I write about random things that comes to my mind!</description><generator>Hugo -- gohugo.io</generator><language>en</language><managingEditor>bodasadallah@gmail.com (Boda Sadallah)</managingEditor><webMaster>bodasadallah@gmail.com (Boda Sadallah)</webMaster><lastBuildDate>Sat, 02 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://bodasadalla98.github.io/blog/index.xml" rel="self" type="application/rss+xml"/><item><title>MIT 18.06 Linear Algebra course</title><link>https://bodasadalla98.github.io/blog/1806_mit/</link><pubDate>Sat, 02 Oct 2021 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/1806_mit/</guid><description><![CDATA[Lecture 1 We learn about the big picture behind multiplication of matrix and vector
we learn about the row picture and column picture
Lecture 2 we learned about elimination method to solve a system of equations
Lecture 3 in this lecture we learned about matrices multiplication:
we can do that in five ways:
row * col ==&gt; gives an entry (1 cell) col _ row ==&gt; sum ( r1 _ c1 , r2 * c2, etc) by columns ==&gt; A * c1 = combination of A columns by columns ==&gt; r1 * B = combination of A B rows by blocks ==&gt; A (A1,A2,A3,A4) _ B (B1,B2,B3,B4) = C1 = (A1_ B1 + A2 * B3) and so on then we learned about gausian-Jordan elimination to find the matrix inverse]]></description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-10-02-1806mit/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-10-02-1806mit/</guid><description>MIT 18.06 Linear Algebra course MIT 18.06 Linear Algebra course
toc: true badges: true comments: true categories: [jupyter,MIT,Linear Algebra, Math] Lecture 1 We learn about the big picture behind mutplication of matrix and vector
we learn about the row picture and column picture
Lecture 2 we learned about elimination method to solve a system of equations
Lecture 3 in this lecture we learned about matrics multiplication:
we can do that in five ways:</description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-10-02-computational-linear-algebra/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-10-02-computational-linear-algebra/</guid><description>This is Post for Computational Linear Algebra Lecture 1 1 import numpy as np 1 2 a = np.array( [[6,5,3,1], [3,6,2,2], [3,4,3,1] ]) b = np.array( [ [1.5 ,1], [2,2.5], [5 ,4.5] ,[16 ,17] ]) 1 2 for c in (a @ b): print(c) [50. 49.] [58.5 61. ] [43.5 43.5] Lecture 2 Matrix decomposition: we decopose matricies into smaller ones that has special properties
Singular Value Decomposition (SVD): it&amp;rsquo;s an exact decomposition, so you can retrieve the orginal matrix again Some SVD applications: semantic analysis collaborative filtering / recommendation data compression PCA (principal component analysis) Non-negative Matrix Factorization (NMF)</description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-10-06-fastai-2020/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-10-06-fastai-2020/</guid><description>FastAi 2020 A summary for FastAi Practical deep learning for coders course.
toc: true badges: true comments: true categories: [jupyter,FastAi] Lecture two P value: determines if some numbers have realationship, or they are random (whether they are independat or dependant) suppose we have the temp and R (transmitity) values of a 100 cities in China and we want to see if there's a relation between them. then we generate many sets of random numbers for each parameter then we calculate the P value which would tell us what's the percentage this slope is a random, and that ther's no relation A P-value is the probability of an observed result assuming that the null hypothesis (there's no relation ) is true PS: P-value also is dependant on the size of the set u used, so they don't measure the importance of the result.</description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-10-06-the-science-of-well-being/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-10-06-the-science-of-well-being/</guid><description>The Science of Well Being Misconcepions about happiness Why our expectations are so bad How can we overcome our biases What&amp;rsquo;s really makes us happy Putting strategies into practice</description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-10-14-text-to-speech/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-10-14-text-to-speech/</guid><description>TTS Research A summary research for TTS
toc: true badges: true comments: true categories: [TTS,Deeplearning] TTS TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion.</description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-10-27-scribbles/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-10-27-scribbles/</guid><description><![CDATA[Scribbles Notebook A bunch of very different topics scribbles
toc: true badges: true comments: true categories: [jupyter,deeplearning,python] Transformers To deal with sequential data we have to options:
1-D convolution NN processing can be parallel not practical for long sequences Recurrent NN can&rsquo;t happen in prallel have gradient vanshing problem of the squence becomes so long we have bottleneck at the end of the encoder RNN with attention mechanism to solve the bottleneck problem, we make Encoder-Decoder attention Decoder utilzes: context vector weighted sum of hidden states (h1,h2, &hellip; ) from the encoder Transformers Encoder first we do input embedding, and positional embedding in self attention: we multiply q,w,v by a matrix to do lenear transformation self attentoion: k * q &ndash;&gt; scaling down &ndash;&gt; softmax &ndash;&gt; * v multi-head attention works as we use many filters in CNN in wide attention: it takes every word and spread it multi-head attention in narrow attention: it take every word and split it up across the multi-head but didnt we lose the adcantage of using multi-head as mutli prespectives, as we do with filters in CNN?]]></description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-11-10-python/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-11-10-python/</guid><description>Python Notebook Python trics
toc: true badges: true comments: true categories: [jupyter,python] Inheritance If you have inherited from parent class then you should call the parent class constructor if you overload it, or simply doesn&amp;rsquo;t overload it Ex:
1 2 3 4 5 6 7 8 9 10 class parent: def __init__: class child: def __init__: super().__init__() class child2: ## or simply don&amp;#39;t override the constructor and use the parent one Multiple Inheritance when we inherit from two or more classes, whatever class we inherited first(typed first in the list), would be the one to have pariority</description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-11-21-voiceconversion/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-11-21-voiceconversion/</guid><description>Voice Conversion A Research notebook, for Voice conversion models
toc: true badges: true comments: true categories: [jupyter,DeepLearning, VoiceConversion] AutoVC paper Demo Repo Speach Split Paper Demo repo AutoPST Paper Demo repo Sources IMPROVING ZERO-SHOT VOICE STYLE TRANSFER VIA DISENTANGLED REPRESENTATION LEARNING Voice Conversion Challenge 2020 results</description></item><item><title/><link>https://bodasadalla98.github.io/blog/2021-11-7-univnet/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><author>Boda Sadallah</author><guid>https://bodasadalla98.github.io/blog/2021-11-7-univnet/</guid><description>Univnet Research for wavegan
toc: true badges: true comments: true categories: [jupyter,deeplearning,python,TTS] TTS (Text To Speech) TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion.</description></item></channel></rss>